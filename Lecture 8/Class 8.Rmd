---
title: "Bioinformatics Class 8"
author: "Peng Fei Huang"
date: "4/27/2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering example 

Lets make up some data for testing the kmeans() function 

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```



```{r}
km <- kmeans(x, centers=2, nstart=20)
km
```


Q. How many points are in each cluster? 

```{r}
km$size
```

Q. What ‘component’ of your result object details
 - cluster size?
 - cluster assignment/membership?
 - cluster center?
 
cluster size
```{r}
km$cluster
```

cluster center 
```{r}
km$centers
```

Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, col=km$cluster, pch=16)
points(km$centers, col="blue", pch=15)
```

#Hierarchical clustering 
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc 
```{r}
dist_matrix <- dist(x)
```
```{r}
class(dist_matrix)
```
Convert to matrix to see the structure of this distance matrix and find the dimensions 

```{r}
dim(as.matrix(dist_matrix))
```
# The hclust() function returns a hierarchical
# clustering model
# the print method is not so useful here
```{r}
hc <- hclust(d = dist_matrix)
hc
```
```{r}
class(hc)
```

```{r}
plot(hc)
abline(h=6, col="red")
```


lets cut our tree to define our clusters 

```{r}
grps <- cutree(hc, h=6)
table(grps)
```


```{r}
plot(x, col=grps)
```

try different cutting 
```{r}
plot(x, col=cutree(hc, k=4))
```

## another hclust example...
Lets make up some more realistic data

# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)

```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```
Lets plot x and color points by our 3 known grps
```{r}
plot(x)
```

```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```


#Lets try hclust() on this data 
```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)

```

```{r}
grps <- cutree(hc, k=3)
table(grps)
```

Plot the data colored by cluster 
```{r}
plot(x, col=grps)
```

#Principal component analysis (PCA)

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1)

head(mydata)
```

```{r}
#View(t(mydata))
```

##Lets do PCA
```{r}
pca <- prcomp(t(mydata), scale=TRUE) 

## see what is retuned by the prcomp() function 
attributes(pca)
```

```{r}
pca
```
```{r}
## A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2])
## Variance captured per PC
pca.var <- pca$sdev^2 
```


```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per
```

```{r}
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```

#Color up our PCA plot


```{r}
mycols <- as.factor(substr(colnames(mydata), 1,2))
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1", ylab="PC2")

```


```{r}
paste("PC1 (",pca.var.per[1],"%)")

```





```{r}
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 

text(pca$x[,1], pca$x[,2], labels=colnames(mydata), pos=1)
```



```{r}
#x <- read.csv("UK.foods.csv")
```
















